{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42sS-Qu6A95o"
      },
      "source": [
        "# Comparing Prompts on MMLU with Statistical Testing\n",
        "\n",
        "A quick end-to-end experiment: two prompts, 30 MMLU questions, and proper significance testing to see if the difference actually matters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdM3mmdfA95u"
      },
      "source": [
        "## 1. Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "80WtNND8A95v"
      },
      "outputs": [],
      "source": [
        "!pip install datasets openai numpy scipy -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMcUnx92A95x"
      },
      "source": [
        "## 2. API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MasyLcrpA95x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d8de01-0526-4003-bd73-45d5c07d0621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Enter your API key (it won't be displayed)\n",
        "api_key = getpass(\"Enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFvqZyEuA95y"
      },
      "source": [
        "## 3. Statistical Functions\n",
        "\n",
        "These are pulled from the `rigor` package — defined inline here so nothing extra to install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5QOdgmR-A95z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Literal, Union\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# RIGOR: Statistical functions for LLM experiments\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class ComparisonResult:\n",
        "    \"\"\"Result of comparing two sets of scores.\"\"\"\n",
        "    test_name: str\n",
        "    statistic: float\n",
        "    p_value: float\n",
        "    significant: bool\n",
        "    effect_size: float\n",
        "    effect_size_interpretation: str\n",
        "    ci_low: float\n",
        "    ci_high: float\n",
        "    mean_a: float\n",
        "    mean_b: float\n",
        "    mean_difference: float\n",
        "    n_samples: int\n",
        "    alpha: float\n",
        "\n",
        "    def __repr__(self):\n",
        "      sig_str = \"significant\" if self.significant else \"not significant\"\n",
        "      return (\n",
        "          f\"\\n{self.test_name}\\n\"\n",
        "          f\"{'-'*50}\\n\"\n",
        "          f\"p-value: {self.p_value:.4f} ({sig_str} at alpha={self.alpha})\\n\"\n",
        "          f\"\\n\"\n",
        "          f\"Prompt A: {self.mean_a:.1%}\\n\"\n",
        "          f\"Prompt B: {self.mean_b:.1%}\\n\"\n",
        "          f\"Difference: {self.mean_difference:.1%} [95% CI: {self.ci_low:.1%}, {self.ci_high:.1%}]\\n\"\n",
        "          f\"\\n\"\n",
        "          f\"Effect size (Cohen's d): {self.effect_size:.3f} ({self.effect_size_interpretation})\\n\"\n",
        "          f\"n = {self.n_samples}\"\n",
        "      )\n",
        "\n",
        "@dataclass\n",
        "class ConfidenceInterval:\n",
        "    \"\"\"Confidence interval result.\"\"\"\n",
        "    estimate: float\n",
        "    ci_low: float\n",
        "    ci_high: float\n",
        "    confidence_level: float\n",
        "    method: str\n",
        "    n_samples: int\n",
        "\n",
        "    @property\n",
        "    def margin_of_error(self):\n",
        "        return (self.ci_high - self.ci_low) / 2\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.estimate:.1%} [{self.ci_low:.1%}, {self.ci_high:.1%}] (n={self.n_samples})\"\n",
        "\n",
        "\n",
        "def bootstrap_ci(data, n_bootstrap=10000, alpha=0.05, random_state=42):\n",
        "    data = np.asarray(data)\n",
        "    n = len(data)\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    boot_stats = np.array([np.mean(rng.choice(data, size=n, replace=True)) for _ in range(n_bootstrap)])\n",
        "    return np.percentile(boot_stats, 100 * alpha / 2), np.percentile(boot_stats, 100 * (1 - alpha / 2))\n",
        "\n",
        "\n",
        "def compute_ci(data, method=\"wilson\", alpha=0.05, n_bootstrap=10000):\n",
        "    data = np.asarray(data)\n",
        "    n = len(data)\n",
        "    if method == \"wilson\":\n",
        "        p_hat = np.mean(data)\n",
        "        z = stats.norm.ppf(1 - alpha / 2)\n",
        "        denom = 1 + z**2 / n\n",
        "        center = (p_hat + z**2 / (2 * n)) / denom\n",
        "        margin = (z / denom) * np.sqrt(p_hat * (1 - p_hat) / n + z**2 / (4 * n**2))\n",
        "        return center - margin, center + margin\n",
        "    elif method == \"bootstrap\":\n",
        "        return bootstrap_ci(data, n_bootstrap=n_bootstrap, alpha=alpha)\n",
        "\n",
        "\n",
        "def cohens_d(scores_a, scores_b):\n",
        "    diff = scores_a - scores_b\n",
        "    std = np.std(diff, ddof=1)\n",
        "    return np.mean(diff) / std if std > 0 else 0.0\n",
        "\n",
        "\n",
        "def interpret_effect_size(d):\n",
        "    d = abs(d)\n",
        "    if d < 0.2: return \"negligible\"\n",
        "    elif d < 0.5: return \"small\"\n",
        "    elif d < 0.8: return \"medium\"\n",
        "    else: return \"large\"\n",
        "\n",
        "\n",
        "def compare_prompts(\n",
        "    scores_a: Union[List[float], np.ndarray],\n",
        "    scores_b: Union[List[float], np.ndarray],\n",
        "    test: Literal[\"paired_t\", \"wilcoxon\", \"bootstrap\", \"mcnemar\", \"auto\"] = \"auto\",\n",
        "    alpha: float = 0.05,\n",
        "    n_bootstrap: int = 10000,\n",
        ") -> ComparisonResult:\n",
        "    \"\"\"\n",
        "    Compare two sets of LLM evaluation scores.\n",
        "\n",
        "    For binary scores (0/1), McNemar's test is most appropriate.\n",
        "    \"\"\"\n",
        "    scores_a = np.asarray(scores_a)\n",
        "    scores_b = np.asarray(scores_b)\n",
        "    n = len(scores_a)\n",
        "\n",
        "    # Check if binary\n",
        "    is_binary = set(np.unique(scores_a)).issubset({0, 1}) and set(np.unique(scores_b)).issubset({0, 1})\n",
        "\n",
        "    if test == \"auto\":\n",
        "        if is_binary:\n",
        "            test = \"mcnemar\"\n",
        "        elif n < 20:\n",
        "            test = \"bootstrap\"\n",
        "        else:\n",
        "            test = \"wilcoxon\"\n",
        "\n",
        "    if test == \"mcnemar\":\n",
        "        # McNemar's test for paired binary data\n",
        "        # Count discordant pairs\n",
        "        b = np.sum((scores_a == 1) & (scores_b == 0))  # A correct, B wrong\n",
        "        c = np.sum((scores_a == 0) & (scores_b == 1))  # A wrong, B correct\n",
        "\n",
        "        if b + c == 0:\n",
        "            p_value = 1.0\n",
        "            stat = 0.0\n",
        "        else:\n",
        "            # McNemar with continuity correction\n",
        "            stat = (abs(b - c) - 1)**2 / (b + c)\n",
        "            p_value = 1 - stats.chi2.cdf(stat, df=1)\n",
        "\n",
        "        test_name = \"McNemar's test\"\n",
        "        ci_low, ci_high = bootstrap_ci(scores_a - scores_b, n_bootstrap=n_bootstrap, alpha=alpha)\n",
        "\n",
        "    elif test == \"wilcoxon\":\n",
        "        diff = scores_a - scores_b\n",
        "        if np.all(diff == 0):\n",
        "            stat, p_value = 0.0, 1.0\n",
        "        else:\n",
        "            stat, p_value = stats.wilcoxon(scores_a, scores_b, zero_method='wilcox')\n",
        "        test_name = \"Wilcoxon signed-rank\"\n",
        "        ci_low, ci_high = bootstrap_ci(scores_a - scores_b, n_bootstrap=n_bootstrap, alpha=alpha)\n",
        "\n",
        "    elif test == \"paired_t\":\n",
        "        stat, p_value = stats.ttest_rel(scores_a, scores_b)\n",
        "        test_name = \"Paired t-test\"\n",
        "        diff = scores_a - scores_b\n",
        "        se = stats.sem(diff)\n",
        "        t_crit = stats.t.ppf(1 - alpha / 2, df=n - 1)\n",
        "        ci_low, ci_high = np.mean(diff) - t_crit * se, np.mean(diff) + t_crit * se\n",
        "\n",
        "    elif test == \"bootstrap\":\n",
        "        diff = scores_a - scores_b\n",
        "        observed_diff = np.mean(diff)\n",
        "        centered_diff = diff - observed_diff\n",
        "        rng = np.random.default_rng(42)\n",
        "        boot_diffs = np.array([np.mean(rng.choice(centered_diff, size=n, replace=True)) for _ in range(n_bootstrap)])\n",
        "        p_value = np.mean(np.abs(boot_diffs) >= np.abs(observed_diff))\n",
        "        stat = observed_diff\n",
        "        test_name = \"Bootstrap\"\n",
        "        ci_low, ci_high = bootstrap_ci(diff, n_bootstrap=n_bootstrap, alpha=alpha)\n",
        "\n",
        "    effect = cohens_d(scores_a, scores_b)\n",
        "    effect_interp = interpret_effect_size(effect)\n",
        "\n",
        "    return ComparisonResult(\n",
        "        test_name=test_name,\n",
        "        statistic=float(stat),\n",
        "        p_value=float(p_value),\n",
        "        significant=bool(p_value < alpha),\n",
        "        effect_size=float(effect),\n",
        "        effect_size_interpretation=effect_interp,\n",
        "        ci_low=float(ci_low),\n",
        "        ci_high=float(ci_high),\n",
        "        mean_a=float(np.mean(scores_a)),\n",
        "        mean_b=float(np.mean(scores_b)),\n",
        "        mean_difference=float(np.mean(scores_a) - np.mean(scores_b)),\n",
        "        n_samples=n,\n",
        "        alpha=alpha,\n",
        "    )\n",
        "\n",
        "\n",
        "def metric_ci(scores, confidence=0.95):\n",
        "    \"\"\"Get confidence interval for accuracy.\"\"\"\n",
        "    scores = np.asarray(scores)\n",
        "    alpha = 1 - confidence\n",
        "    ci_low, ci_high = compute_ci(scores, method=\"wilson\", alpha=alpha)\n",
        "    return ConfidenceInterval(\n",
        "        estimate=float(np.mean(scores)),\n",
        "        ci_low=float(ci_low),\n",
        "        ci_high=float(ci_high),\n",
        "        confidence_level=confidence,\n",
        "        method=\"wilson\",\n",
        "        n_samples=len(scores),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUgBgTL1A950"
      },
      "source": [
        "## 4. Load MMLU\n",
        "\n",
        "Pulling 30 questions across 8 subjects — a mix of high school and college level to keep it representative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mj6bwMcqA951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b31625a-6ae8-46e1-a272-c8e9278a8739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 questions loaded\n",
            "\n",
            "Subjects included:\n",
            "  - high_school_mathematics: 4\n",
            "  - high_school_physics: 4\n",
            "  - high_school_chemistry: 4\n",
            "  - high_school_biology: 4\n",
            "  - high_school_computer_science: 4\n",
            "  - college_mathematics: 4\n",
            "  - college_physics: 4\n",
            "  - machine_learning: 2\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Choose subjects to sample from\n",
        "SUBJECTS = [\n",
        "    \"high_school_mathematics\",\n",
        "    \"high_school_physics\",\n",
        "    \"high_school_chemistry\",\n",
        "    \"high_school_biology\",\n",
        "    \"high_school_computer_science\",\n",
        "    \"college_mathematics\",\n",
        "    \"college_physics\",\n",
        "    \"machine_learning\",\n",
        "]\n",
        "\n",
        "# Load questions from multiple subjects\n",
        "N_EXAMPLES = 30\n",
        "examples_per_subject = N_EXAMPLES // len(SUBJECTS) + 1\n",
        "\n",
        "all_examples = []\n",
        "for subject in SUBJECTS:\n",
        "    try:\n",
        "        ds = load_dataset(\"cais/mmlu\", subject, split=f\"test[:{examples_per_subject}]\")\n",
        "        for ex in ds:\n",
        "            ex['subject'] = subject\n",
        "            all_examples.append(ex)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load {subject}: {e}\")\n",
        "\n",
        "# Take exactly N_EXAMPLES\n",
        "dataset = all_examples[:N_EXAMPLES]\n",
        "\n",
        "print(f\"{len(dataset)} questions loaded\")\n",
        "print(f\"\\nSubjects included:\")\n",
        "from collections import Counter\n",
        "subject_counts = Counter(ex['subject'] for ex in dataset)\n",
        "for subj, count in subject_counts.items():\n",
        "    print(f\"  - {subj}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XI5TxSIGA952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5c902b-4841-409a-9d5b-287be6cf7998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: high_school_mathematics\n",
            "Question: If a pentagon P with vertices at (– 2, – 4), (– 4, 1), (–1, 4), (2, 4), and (3, 0) is reflected across the line y = x to get a new pentagon, P’, then one of the vertices of P’ is\n",
            "  A. (0, – 3)\n",
            "  B. (4, 1)\n",
            "  C. (2, 2)\n",
            "  D. (– 4, –2)\n",
            "Correct answer: D\n"
          ]
        }
      ],
      "source": [
        "# Sanity check — look at one question\n",
        "ex = dataset[0]\n",
        "print(f\"Subject: {ex['subject']}\")\n",
        "print(f\"Question: {ex['question']}\")\n",
        "for i, choice in enumerate(ex['choices']):\n",
        "    label = chr(65 + i)  # A, B, C, D\n",
        "    print(f\"  {label}. {choice}\")\n",
        "print(f\"Correct answer: {chr(65 + ex['answer'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDvWCo8iA953"
      },
      "source": [
        "## 5. The Two Prompts\n",
        "\n",
        "Prompt A asks the model to reason before answering. Prompt B goes straight to the answer. Simple comparison - does chain-of-thought actually help here?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qaO2n5dLA953",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0d0df9-dc3d-4b90-a86b-7ff5ff173885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt A (with reasoning):\n",
            "Answer this multiple choice question. Think through it step by step, then give your final answer as a single letter (A, B, C, or D).\n",
            "\n",
            "Question: {quest...\n",
            "\n",
            "Prompt B (direct answer):\n",
            "Answer this multiple choice question with just the letter (A, B, C, or D).\n",
            "\n",
            "Question: {question}\n",
            "\n",
            "A. {choice_a}\n",
            "B. {choice_b}\n",
            "C. {choice_c}\n",
            "D. {choice...\n"
          ]
        }
      ],
      "source": [
        "# The two prompts we're comparing\n",
        "\n",
        "PROMPT_A = \"\"\"Answer this multiple choice question. Think through it step by step, then give your final answer as a single letter (A, B, C, or D).\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "A. {choice_a}\n",
        "B. {choice_b}\n",
        "C. {choice_c}\n",
        "D. {choice_d}\n",
        "\n",
        "Think step by step, then answer:\"\"\"\n",
        "\n",
        "PROMPT_B = \"\"\"Answer this multiple choice question with just the letter (A, B, C, or D).\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "A. {choice_a}\n",
        "B. {choice_b}\n",
        "C. {choice_c}\n",
        "D. {choice_d}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(\"Prompt A (with reasoning):\")\n",
        "print(PROMPT_A[:150] + \"...\")\n",
        "print(\"\\nPrompt B (direct answer):\")\n",
        "print(PROMPT_B[:150] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEYGlD4dA954"
      },
      "source": [
        "## 6. Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a9yby_cVA954"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def format_prompt(template, example):\n",
        "    \"\"\"Format a prompt template with the question and choices.\"\"\"\n",
        "    return template.format(\n",
        "        question=example['question'],\n",
        "        choice_a=example['choices'][0],\n",
        "        choice_b=example['choices'][1],\n",
        "        choice_c=example['choices'][2],\n",
        "        choice_d=example['choices'][3],\n",
        "    )\n",
        "\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"\n",
        "    Extract the answer letter (A, B, C, or D) from model response.\n",
        "    Looks for patterns like \"A\", \"(A)\", \"Answer: A\", etc.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    text = text.strip().upper()\n",
        "\n",
        "    # Try to find explicit answer patterns first\n",
        "    patterns = [\n",
        "        r'(?:ANSWER|FINAL ANSWER)[:\\s]*([ABCD])',\n",
        "        r'(?:THE ANSWER IS)[:\\s]*([ABCD])',\n",
        "        r'\\(([ABCD])\\)',\n",
        "        r'^([ABCD])[\\.\\)\\s]',\n",
        "        r'([ABCD])$',\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "\n",
        "    # Last resort: find any A, B, C, D in the last part of response\n",
        "    last_part = text[-50:] if len(text) > 50 else text\n",
        "    for char in reversed(last_part):\n",
        "        if char in 'ABCD':\n",
        "            return char\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def run_prompt(prompt_text, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Run a prompt through the model and return the response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
        "            max_tokens=300,\n",
        "            temperature=0,  # Deterministic for reproducibility\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_correct_letter(example):\n",
        "    \"\"\"Get the correct answer letter from the example.\"\"\"\n",
        "    return chr(65 + example['answer'])  # 0->A, 1->B, 2->C, 3->D\n",
        "\n",
        "\n",
        "def score_response(response, correct_letter):\n",
        "    \"\"\"\n",
        "    Score a response: 1 if correct, 0 if incorrect.\n",
        "    \"\"\"\n",
        "    extracted = extract_answer(response)\n",
        "    if extracted is None:\n",
        "        return 0\n",
        "    return 1 if extracted == correct_letter else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_9FgQ7VA955"
      },
      "source": [
        "## 7. Run the Experiment\n",
        "\n",
        "Same question, both prompts. Paired design — this matters for the test selection later.\n",
        "\n",
        "Should take 2–3 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "66TcLmlKA955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21edb6a0-65be-4b0f-cbd4-22579be2af23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 30 questions...\n",
            "\n",
            "[ 1/30] high_school_mat | A:C B:C (correct: D)\n",
            "[ 2/30] high_school_mat | A:C B:A (correct: C)\n",
            "[ 3/30] high_school_mat | A:A B:B (correct: A)\n",
            "[ 4/30] high_school_mat | A:C B:B (correct: B)\n",
            "[ 5/30] high_school_phy | A:C B:B (correct: B)\n",
            "[ 6/30] high_school_phy | A:B B:C (correct: A)\n",
            "[ 7/30] high_school_phy | A:D B:D (correct: D)\n",
            "[ 8/30] high_school_phy | A:B B:A (correct: C)\n",
            "[ 9/30] high_school_che | A:A B:A (correct: A)\n",
            "[10/30] high_school_che | A:A B:B (correct: A)\n",
            "[11/30] high_school_che | A:C B:C (correct: C)\n",
            "[12/30] high_school_che | A:B B:A (correct: A)\n",
            "[13/30] high_school_bio | A:A B:A (correct: A)\n",
            "[14/30] high_school_bio | A:C B:B (correct: A)\n",
            "[15/30] high_school_bio | A:C B:C (correct: C)\n",
            "[16/30] high_school_bio | A:A B:A (correct: A)\n",
            "[17/30] high_school_com | A:C B:C (correct: C)\n",
            "[18/30] high_school_com | A:A B:A (correct: A)\n",
            "[19/30] high_school_com | A:A B:A (correct: A)\n",
            "[20/30] high_school_com | A:B B:B (correct: C)\n",
            "[21/30] college_mathema | A:B B:A (correct: B)\n",
            "[22/30] college_mathema | A:B B:C (correct: D)\n",
            "[23/30] college_mathema | A:D B:D (correct: D)\n",
            "[24/30] college_mathema | A:C B:D (correct: A)\n",
            "[25/30] college_physics | A:B B:A (correct: B)\n",
            "[26/30] college_physics | A:D B:B (correct: C)\n",
            "[27/30] college_physics | A:C B:C (correct: C)\n",
            "[28/30] college_physics | A:A B:A (correct: A)\n",
            "[29/30] machine_learnin | A:C B:C (correct: D)\n",
            "[30/30] machine_learnin | A:C B:C (correct: C)\n",
            "\n",
            "Prompt A (reasoning): 18/30 correct (60.0%)\n",
            "Prompt B (direct):    16/30 correct (53.3%)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Store results\n",
        "scores_a = []  # Reasoning prompt\n",
        "scores_b = []  # Direct prompt\n",
        "results_log = []  # Detailed log\n",
        "\n",
        "print(f\"Running {len(dataset)} questions...\\n\")\n",
        "\n",
        "for i, example in enumerate(dataset):\n",
        "    correct = get_correct_letter(example)\n",
        "\n",
        "    # Format and run both prompts\n",
        "    prompt_a_text = format_prompt(PROMPT_A, example)\n",
        "    prompt_b_text = format_prompt(PROMPT_B, example)\n",
        "\n",
        "    response_a = run_prompt(prompt_a_text)\n",
        "    response_b = run_prompt(prompt_b_text)\n",
        "\n",
        "    # Extract answers\n",
        "    answer_a = extract_answer(response_a)\n",
        "    answer_b = extract_answer(response_b)\n",
        "\n",
        "    # Score responses\n",
        "    score_a = score_response(response_a, correct)\n",
        "    score_b = score_response(response_b, correct)\n",
        "\n",
        "    scores_a.append(score_a)\n",
        "    scores_b.append(score_b)\n",
        "\n",
        "    # Log details\n",
        "    results_log.append({\n",
        "        \"subject\": example['subject'],\n",
        "        \"question\": example['question'][:60] + \"...\",\n",
        "        \"correct\": correct,\n",
        "        \"answer_a\": answer_a,\n",
        "        \"answer_b\": answer_b,\n",
        "        \"score_a\": score_a,\n",
        "        \"score_b\": score_b,\n",
        "    })\n",
        "\n",
        "    # Progress indicator\n",
        "    subj_short = example['subject'][:15]\n",
        "    print(f\"[{i+1:2d}/{len(dataset)}] {subj_short:15} | A:{answer_a or '?'} B:{answer_b or '?'} (correct: {correct})\")\n",
        "\n",
        "    # Small delay to avoid rate limits\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(f\"\\nPrompt A (reasoning): {sum(scores_a)}/{len(scores_a)} correct ({sum(scores_a)/len(scores_a):.1%})\")\n",
        "print(f\"Prompt B (direct):    {sum(scores_b)}/{len(scores_b)} correct ({sum(scores_b)/len(scores_b):.1%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmMM93QUA955"
      },
      "source": [
        "## 8. Statistical Analysis\n",
        "\n",
        "Binary scores (correct/incorrect) on paired data — McNemar's test is the right call here. The package picks this automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7rYq18atA956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0df001-f59a-40db-8677-5555b6c38680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "McNemar's test\n",
            "--------------------------------------------------\n",
            "p-value: 0.7237 (not significant at alpha=0.05)\n",
            "\n",
            "Prompt A: 60.0%\n",
            "Prompt B: 53.3%\n",
            "Difference: 6.7% [95% CI: -13.3%, 23.3%]\n",
            "\n",
            "Effect size (Cohen's d): 0.128 (negligible)\n",
            "n = 30\n"
          ]
        }
      ],
      "source": [
        "# THE KEY QUESTION: Is Prompt A actually better than Prompt B?\n",
        "result = compare_prompts(scores_a, scores_b)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2vXaXlzA956"
      },
      "source": [
        "## 9. Confidence Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uZ2oKPm3A956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddc81f85-2689-48bb-ba95-6020a1cf62f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual Accuracy with 95% Confidence Intervals:\n",
            "=======================================================\n",
            "Prompt A (reasoning): 60.0% [42.3%, 75.4%] (n=30)\n",
            "Prompt B (direct):    53.3% [36.1%, 69.8%] (n=30)\n",
            "=======================================================\n"
          ]
        }
      ],
      "source": [
        "ci_a = metric_ci(scores_a)\n",
        "ci_b = metric_ci(scores_b)\n",
        "\n",
        "print(\"Individual Accuracy with 95% Confidence Intervals:\")\n",
        "print(\"=\"*55)\n",
        "print(f\"Prompt A (reasoning): {ci_a}\")\n",
        "print(f\"Prompt B (direct):    {ci_b}\")\n",
        "print(\"=\"*55)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f22nEqs0A956"
      },
      "source": [
        "## 10. By Subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zp4VIjofA957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2540e6f-d5eb-4bbd-d0ca-77afb0b6c935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy by Subject:\n",
            "======================================================================\n",
            "Subject                             |   Prompt A |   Prompt B |     Diff\n",
            "----------------------------------------------------------------------\n",
            "college_mathematics                 |        50% |        25% |     +25%\n",
            "college_physics                     |        75% |        50% |     +25%\n",
            "high_school_biology                 |        75% |        75% |       0%\n",
            "high_school_chemistry               |        75% |        75% |       0%\n",
            "high_school_computer_science        |        75% |        75% |       0%\n",
            "high_school_mathematics             |        50% |        25% |     +25%\n",
            "high_school_physics                 |        25% |        50% |     -25%\n",
            "machine_learning                    |        50% |        50% |       0%\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Break down by subject\n",
        "print(\"\\nAccuracy by Subject:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Subject':<35} | {'Prompt A':>10} | {'Prompt B':>10} | {'Diff':>8}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "from collections import defaultdict\n",
        "subject_scores = defaultdict(lambda: {'a': [], 'b': []})\n",
        "\n",
        "for log in results_log:\n",
        "    subject_scores[log['subject']]['a'].append(log['score_a'])\n",
        "    subject_scores[log['subject']]['b'].append(log['score_b'])\n",
        "\n",
        "for subject, scores in sorted(subject_scores.items()):\n",
        "    acc_a = sum(scores['a']) / len(scores['a'])\n",
        "    acc_b = sum(scores['b']) / len(scores['b'])\n",
        "    diff = acc_a - acc_b\n",
        "    diff_str = f\"+{diff:.0%}\" if diff > 0 else f\"{diff:.0%}\"\n",
        "    print(f\"{subject:<35} | {acc_a:>10.0%} | {acc_b:>10.0%} | {diff_str:>8}\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVpukRExA957"
      },
      "source": [
        "## 11. Detailed Disagreements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Gj9lXDTQA957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23157428-7e66-4108-fb8c-37d717889615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cases where prompts disagreed:\n",
            "======================================================================\n",
            "Q2 [high_school_mathemat]\n",
            "   The length of a rectangle is twice its width. Given the leng...\n",
            "   Correct: C | A: C | B: A → A wins\n",
            "\n",
            "Q3 [high_school_mathemat]\n",
            "   A positive integer n is called “powerful” if, for every prim...\n",
            "   Correct: A | A: A | B: B → A wins\n",
            "\n",
            "Q4 [high_school_mathemat]\n",
            "   At breakfast, lunch, and dinner, Joe randomly chooses with e...\n",
            "   Correct: B | A: C | B: B → B wins\n",
            "\n",
            "Q5 [high_school_physics]\n",
            "   The plates of a capacitor are charged to a potential differe...\n",
            "   Correct: B | A: C | B: B → B wins\n",
            "\n",
            "Q10 [high_school_chemistr]\n",
            "   Carbon has an atomic radius of 77 pm and a first ionization ...\n",
            "   Correct: A | A: A | B: B → A wins\n",
            "\n",
            "Q12 [high_school_chemistr]\n",
            "   The net ionic equation expected when solutions of NH4Br and ...\n",
            "   Correct: A | A: B | B: A → B wins\n",
            "\n",
            "Q21 [college_mathematics]\n",
            "   Let k be the number of real solutions of the equation e^x + ...\n",
            "   Correct: B | A: B | B: A → A wins\n",
            "\n",
            "Q25 [college_physics]\n",
            "   The quantum efficiency of a photon detector is 0.1. If 100 p...\n",
            "   Correct: B | A: B | B: A → A wins\n",
            "\n",
            "Summary:\n",
            "  - Prompt A wins: 5\n",
            "  - Prompt B wins: 3\n",
            "  - Ties (both right or both wrong): 22\n"
          ]
        }
      ],
      "source": [
        "# Show where the prompts disagreed\n",
        "print(\"\\nCases where prompts disagreed:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "disagreements = 0\n",
        "a_wins = 0\n",
        "b_wins = 0\n",
        "\n",
        "for i, log in enumerate(results_log):\n",
        "    if log['score_a'] != log['score_b']:\n",
        "        disagreements += 1\n",
        "        if log['score_a'] > log['score_b']:\n",
        "            winner = \"A wins\"\n",
        "            a_wins += 1\n",
        "        else:\n",
        "            winner = \"B wins\"\n",
        "            b_wins += 1\n",
        "        print(f\"Q{i+1} [{log['subject'][:20]}]\")\n",
        "        print(f\"   {log['question']}\")\n",
        "        print(f\"   Correct: {log['correct']} | A: {log['answer_a']} | B: {log['answer_b']} → {winner}\")\n",
        "        print()\n",
        "\n",
        "ties = len(results_log) - disagreements\n",
        "print(f\"Summary:\")\n",
        "print(f\"  - Prompt A wins: {a_wins}\")\n",
        "print(f\"  - Prompt B wins: {b_wins}\")\n",
        "print(f\"  - Ties (both right or both wrong): {ties}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJJmZsUwA957"
      },
      "source": [
        "## 12. Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "p8R1i1GJA957",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01794c0e-5c64-4bed-c647-5a63ee4be46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Raw accuracy difference: +6.7%\n",
            "  Prompt A (reasoning): 60.0%\n",
            "  Prompt B (direct):    53.3%\n",
            "\n",
            "The difference is not statistically significant (p = 0.7237)\n",
            "Can't confidently say one prompt is better. Could be noise.\n",
            "With n=30, might need more data to detect a real difference.\n",
            "\n",
            "Decision: Default to the simpler/faster/cheaper prompt (B) unless you can collect more data.\n",
            "\n",
            "Effect size: 0.128 (negligible)\n"
          ]
        }
      ],
      "source": [
        "acc_a = np.mean(scores_a)\n",
        "acc_b = np.mean(scores_b)\n",
        "diff = acc_a - acc_b\n",
        "\n",
        "print(f\"\\nRaw accuracy difference: {diff:+.1%}\")\n",
        "print(f\"  Prompt A (reasoning): {acc_a:.1%}\")\n",
        "print(f\"  Prompt B (direct):    {acc_b:.1%}\")\n",
        "\n",
        "if result.significant:\n",
        "    better = \"A (reasoning)\" if diff > 0 else \"B (direct)\"\n",
        "    print(f\"\\nThe difference is statistically significant (p = {result.p_value:.4f})\")\n",
        "    print(f\"Prompt {better} appears genuinely better — unlikely to be random chance.\")\n",
        "    print(f\"Decision: Deploy prompt {better}.\")\n",
        "else:\n",
        "    print(f\"\\nThe difference is not statistically significant (p = {result.p_value:.4f})\")\n",
        "    print(f\"Can't confidently say one prompt is better. Could be noise.\")\n",
        "    print(f\"With n={len(scores_a)}, might need more data to detect a real difference.\")\n",
        "    print(f\"\\nDecision: Default to the simpler/faster/cheaper prompt (B) unless you can collect more data.\")\n",
        "\n",
        "print(f\"\\nEffect size: {result.effect_size:.3f} ({result.effect_size_interpretation})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYBFdzb1A958"
      },
      "source": [
        "## Takeaway\n",
        "\n",
        "The raw accuracy gap might look meaningful at first glance, but the statistics tell you whether it actually is.\n",
        "\n",
        "With p > 0.05 here, the 6.7% difference could easily be noise on 30 questions. You'd need a larger sample to say anything definitive.\n",
        "\n",
        "**Why n=30?** This is deliberately small to demonstrate the point: eyeballing a 6.7% difference feels meaningful, but the test reveals we can't distinguish it from chance. In production, you'd run 100-300 samples — this notebook is meant to show the methodology, not be a definitive MMLU evaluation.\n",
        "\n",
        "Want to tighten this up? Bump `N_EXAMPLES` to 100+ and rerun. Or use `sample_size()` from the rigor package to figure out exactly how many you need upfront.\n",
        "\n",
        "**Note:** MMLU has clear right/wrong answers, which makes it easy to demonstrate the methodology. Real-world evaluation (RAG, open-ended generation, subjective quality) requires the same statistical rigor but with more complex scoring — that's where this approach becomes even more critical.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}